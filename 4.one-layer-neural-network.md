# 4. 理论：单层神经网络

![Softmax Cross-entropy Mini-batch](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/bdd8f1f362889583.png)

MNIST 数据集中，手写数字是 28x28 像素的灰度图像。将它们进行分类的最简单的方法就是使用 28x28=784 个像素作为单层神经网络的输入。


![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/d5222c6e3d15770a.png)

神经网络中的每个**神经元**对其所有的输入进行加权求和，并添加一个被称为**偏置（bias）**的常数，然后通过一些非线性激活函数来反馈结果。

为了将数字分为 10 类（0 到 9），我们设计了一个具有 10 个输出神经元的单层神经网络。对于分类问题，softmax 是一个不错的激活函数。通过取各个元素的指数，然后把向量归一化（使用任意的范数（norm），比如向量的普通欧几里得距离）从而将 softmax 应用于向量。

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/604a9797da2a48d7.png)


> 那么为什么 “softmax” 会被称为 softmax 呢？指数是一种骤增的函数，这将扩大向量中每个元素的差异。它也会迅速地产生一个巨大的值。之后，当进行向量的标准化时，最大的元素、也就是主导（dominate）了范数（norm）的那个元素，将会被标准化为一个接近 1 的数字；其他的元素将除以较大的值并，于是被标准化为一个接近 0 的数字。最终得到的向量清楚地显示出了哪个是其最大的值，即 “max”，但是却又保留了其值的原始的相对排列顺序，因此即为 “soft”。

我们现在将利用矩阵乘法，将这个单层的神经元行为总结成一个简单的公式。让我们直接这样做：100 个图像的 mini-batch 作为输入，产生 100 个预测（10 元素向量）作为输出

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/21dabcf6d44e4d6f.png)

使用权值矩阵 W 的第一列权重，计算第一个图像所有像素的加权和。该和对应于第一神经元。使用第二列权重，我们对第二个神经元进行同样的操作，直到第 10 个神经元。然后，我们可以对剩余的 99 个图像重复操作。如果我们把一个包含 100 个图像的矩阵称为 X，那么我们的 10 个神经元在这 100 张图像上的加权和就是简单的 X.W（矩阵乘）。

每一个神经元都必须添加其偏置（常数）。因为我们有 10 个神经元，我们同样拥有 10 个偏置常数。我们将这个 10 个值的向量称为 b。它必须被添加到先前计算的矩阵中的每一行当中。使用一个称为 "broadcasting" 的魔法，我们将会用一个简单的加号写出它

> **Broadcasting** 是 Python 和 numpy（Python 的科学计算库）的一个标准技巧。它扩展了对不兼容维度的矩阵进行正常操作的方式。**Broadcasting 相加**意味着：如果你因为两个矩阵维度不同的原因而不能将其相加，那么你可以根据需要尝试复制那个小的矩阵使其工作。

我们最终应用 softmax 激活函数并且得到一个描述单层神经网络的公式，并将其应用于 100 张图像：

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/206327168bc85294.png)

> 顺便说一下，什么是 “张量（tensor）”？

> **张量（tensor）**像一个矩阵，不同之处是它的维度可以是任意的。一个一维的张量是一个向量，二维的张量是一个矩阵，然后可以有 3, 4, 5 乃至更多维的张量。

