# 5. 理论：梯度下降

现在我们的神经网络从输入图像中产生预测，我们需要知道这个模型有多好，即，事实情况和网络的预测之间到底有多大的距离。回忆下，这个数据集中的所有图像都有一个真实的标签（label）。

任何一种定义的距离都可以进行这样的操作，普通欧几里得距离是可以的，但是对于分类问题，被称为**交叉熵（cross-entropy）**的距离更加有效。


![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/1d8fc59e6a674f1c.png)

> **one-hot** 编码意味着你使用一个 10 个值的向量，其中除了第 6 个值为 1 ，其他值都是 0。这种格式很方便，因为和我们神经网络预测输出的格式非常相似，也是一个 10 值的向量。

**训练**一个神经网络实际上意味着使用训练图像和标签来调整权重和偏置，从而最小化交叉熵损失函数。它是这样工作的：

交叉熵是一个关于权重、偏置、训练图像的像素和其实际标签的函数。

如果我们相对于所有的权重和偏置计算交叉熵的偏导数，我们就得到一个对于给定图像、标签和当前权重和偏置的**梯度**。记住，我们有 7850 个权重和偏置，所以计算梯度需要大量的工作。幸运的是，TensorFlow 可以来帮我们做这项工作。

梯度的数学意义在于它指向**上方**（points up）。因为我们想要到达一个交叉熵低的地方，那么我们就去向相反的方向。我们用一小部分的梯度更新权重和偏置，并用下一批训练图像再次做同样的事情。期望之中，这将使我们到达最凹陷的地方，这里交叉熵最小。

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/34e9e76c7715b719.png)

在这个图像中，交叉熵被表示为一个具有两个权重的函数。事实上要多得多。梯度下降算法遵循着一个最陡的坡度下降到局部最小值的路径。训练图像在每一次迭代中同样会被改变，这使得我们向着一个适用于所有图像的局部最小值收敛。

> **学习率（learning rate）**： 在整个梯度的长度上，你不能在每一次迭代的时候都对权重和偏置进行更新。这就好比你穿了双一步迈七里的鞋子，试图走到谷底——你会直接从山谷的一边飞到另一边。为了到达底部，你需要一些更小的步子，即只使用梯度的一小部分，通常在 1/1000 区域中。我们称这个比率为**学习率（Learning rate）**。

总结一下，以下是训练过程的步骤：

```
训练数字和标签 => 损失函数 => 梯度（偏导数）=> 最陡的梯度 => 更新权重和偏置 => 使用下一个 mini-batch 的图像和标签重复这一过程
```

> 为什么使用 100 个图像的 **mini-batch**？
> 
> 你当然也可以只在一个样本图像中计算你的梯度并且立即更新权重和偏置（这在论文中被称为「随机梯度下降（stochastic gradient descent）」）。但是 100 个样本能更好地表示由不同样本图像施加约束，从而梯度可能更快地朝着正确的解收敛。mini-batch 的大小是可调节的参数。此外，还有一个更加技术化的原因：使用批处理也意味着使用较大的矩阵，而这通常更容易在 GPU 上优化。


### FAQ

[为什么交叉熵是在分类问题中合适的定义距离？](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/)

