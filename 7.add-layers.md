# 7. 实验：增加 Layer

为了提高识别准确度，我们将为神经网络增加更多层（Layer）。第二层神经元将计算前一层神经元输出的加权和，而非计算像素的加权和。这里有一个 5 层全连接神经网络的例子：

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/77bc41f211c9fb29.png)

我们继续用 softmax 来作为最后一层的激活函数，因为它在分类问题中工作的最好。但在中间层，我们要使用最经典的激活函数：sigmoid：

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/e5d46c389470df62.png)

> 在这一节中，你的任务是为模型增加一到两个中间层以提高性能。
>
> 答案可以在 `mnist_2.0_five_layers_sigmoid.py` 中找到。只有当你卡住的时候再看它！

为了增加一个层，你需要为中间层增加额外一个的权重矩阵和一个的偏置向量：

```python
W1 = tf.Variable(tf.truncated_normal([28*28, 200], stddev=0.1))B1 = tf.Variable(tf.zeros([200]))W2 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))B2 = tf.Variable(tf.zeros([10]))
```

权重矩阵的形状是 [N, M]，这里 N 是该层输入值的数量，M 是该层输出值的数量。在上述代码中，我们在中间层放了 200 个神经元，而输出层仍为 10 个神经元。

> **提示**：随着你学习的深入，把权值初始化为随机值越来越重要，否则 optimiser 可能会一直卡在起始的位置。TensorFlow 函数 `tf.truncated_normal` 能生成正态（高斯）分布的随机值，范围在 $-2\sigma$ 和 $+2\sigma$ 之间。

现在，把单层模型改成二层模型：

```python
XX = tf.reshape(X, [-1, 28*28])Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)Y  = tf.nn.softmax(tf.matmul(Y1, W2) + B2)
```

That's it! 你现在可以试试增加到 2 个中间层，比如分别有 200 个和 100 个神经元，那将能把准确率提升到 97% 以上（如果达不到，下一节会告诉你几个有用的改进技巧——译者注）。

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/dbbf4c8edae90438.png)

