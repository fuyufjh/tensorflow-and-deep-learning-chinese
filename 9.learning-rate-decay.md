# 9. 实验：学习率衰减

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/8ec267106683ff35.png)

通过两个、三个或者四个中间层，你现在可以将准确度提升至接近 98%，当然，你的迭代次数要达到 5000 次以上。然而你会发现结果并不一直如此。

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/19b544d307a09804.png)

这些曲线充满噪声，看测试精确度：它在全百分比范围内跳上跳下。这意味着即使 0.003 的学习率我们还是太快了。但我们不能仅仅将学习率除以十或者永远不停地做训练。一个好的解决方案是：一开始很快，随后将学习速率指数级衰减至，比如说，0.0001。

这个小改变的效果惊人。你会看到大部分的噪声消失了并且测试精确度持续稳定在 98% 以上。

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/36c4ff32da84a637.png)

再看看训练精确度曲线。在几个 epoch 里都达到了 100%（一个 epoch = 500 次迭代 = 全部训练图片训练一次）。我们第一次能完美地识别训练图片了。

> 请把学习率衰退加到你的代码里。为了把一个不同的学习率在每次迭代时传给 AdamOptimizer，你需要定义一个新的占位符（placeholder）并在每次迭代时通过 `feed_dict` 赋给它一个新的参数。
> 
> 这里是一个指数级衰减的方程：$r = r_{min} + (r_{max} - r_{min}) e^{-\frac{i}{2000}}$
> 
> 答案可以在这个文件里找到：`mnist_2.1_five_layers_relu_lrdecay.py`，如果你被卡住了可以看看。


![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/9a204ebcd25ed434.png)
