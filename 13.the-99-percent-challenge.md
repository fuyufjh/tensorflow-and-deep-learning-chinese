# 13. 实验：挑战 99% 准确率

调整你的神经网络的一个好方法是：先去实现一个限制较多的神经网络，然后给它更多的自由度并且增加 dropout，使神经网络避免过拟合。最终你将得到一个相当不错的神经网络。

例如，我们在第一层卷积层中仅仅使用了 4 个 patch，如果这些权重的 patch 在训练的过程中发展成不同的识别器，你可以直观地看到这对于解决我们的问题是不够的。手写数字模式远多于 4 种基本样式。

因此，让我们稍微增加 patch 的数量，将我们卷积层中 patch 的数量从 4，8，12 增加到 6，12，24，并且在全连接层上添加 dropout。它们的神经元重复使用相同的权重，在一次训练迭代中，通过冻结（限制）一些不会对它们起作用的权重，dropout 能够有效地工作。

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/3490f252083904d8.png)

> 上吧！去打破 99％的限制。增加 patch 大小和通道数量，并在卷积层中添加 dropout。
>
> 答案可以在 `mnist_3.1_convolutional_bigger_dropout.py` 中找到，如果你卡住了可以看它。

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/59472e85398457c7.png)

使用上图所示的模型，在 10000 个测试的数字中，结果仅仅错误了 72 个。你可以在 MNIST 网站上发现，准确率的世界纪录大约为 99.7%，这仅比我们用 100 行 Python/TensorFlow 代码构建的模型高 0.4%。

作为结尾，以下就是 dropout 为我们更大的卷积网络所带来的改变。给网络的额外自由度，使模型的准确率从 98.9% 达到 99.1%。向卷积层中增加 dropout 不仅令我们驯服了测试误差，而且让我们安全地在 99% 的准确率上“航行”，甚至能达到 99.3%。

![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/e4f164be456fcf92.png)
